meta_variable:
  experiment_name: 'las_example'              # Expriment title, log/checkpoint files will be named after this
  checkpoint_dir: 'checkpoint/'               # Folder for model checkpoints, make sure created before running
  training_log_dir: 'log/'                    # Folder for training logs, make sure created before running
  data_path: 'timit/std_preprocess_26_ch.pkl' # Preprocessed TIMIT data generated by timt_preprocess.sh

model_parameter:
  max_timestep: 784                           # max_timestep%8 == 0 is required due to listener time resolution reduction
  max_label_len: 77                           # 
  input_feature_dim: 26                       # Default feature dimension in the original paper where CTC was proposed
  listener_hidden_dim: 256                    # Default listener LSTM output dimension from LAS paper
  use_mlp_in_attention: True                  # Set to False to exclude phi and psi in attention formula
  mlp_dim_in_attention: 128                   #
  mlp_activate_in_attention: 'relu'           #
  speller_rnn_layer: 1                        # Default RNN layer number 
  speller_hidden_dim: 512                     # Default speller LSTM output dimension from LAS paper
  output_class_dim: 63                        # 61 phonemes + 2 for <sos> & <eos>
  rnn_unit: 'LSTM'                            # Default recurrent unit in the original paper
  use_gpu: True
  bucketing: True                             # Bucket training data by input sequence length


training_parameter:
  learning_rate: 0.0001
  seed: 1                  
  num_epochs: 100
  batch_size: 4
  verbose_step: 20                            # Show progress every verbose_step
  verbose: True                               # If True, the training progress will be printed to stdout
  use_pretrained: False                       # Load a pretrained model
  pretrained_model_path: ''                   # pretrained_model_path should be specified if use_pretrained == True
